{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T00:59:39.390601600Z",
     "start_time": "2026-01-20T00:59:38.335689400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import storage, bigquery\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from monai.transforms import Compose, ScaleIntensity, Resize, EnsureType\n",
    "from monai.networks.nets import DenseNet121\n",
    "from captum.attr import LayerGradCam\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID = 'pneumonia-detection-2026'\n",
    "LOCAL_DATA_DIR = \"./data/nih_images\"\n",
    "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Clients\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"‚úÖ Environment Ready. Running on: {device}\")"
   ],
   "id": "5cb2bf1e6b469e4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment Ready. Running on: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-20T00:59:40.482964700Z",
     "start_time": "2026-01-20T00:59:39.402986600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_metadata_from_bigquery(limit=200):\n",
    "    # This query extracts the filename and creates a binary label for Pneumonia\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            image_id,\n",
    "            IF('Pneumonia' IN UNNEST(SPLIT(finding_labels, '|')), 1, 0) as label,\n",
    "            patient_id\n",
    "        FROM `search.chc-nih-chest-xray.nih_chest_xray.metadata`\n",
    "        LIMIT {limit}\n",
    "    \"\"\"\n",
    "    return bq_client.query(query).to_dataframe()\n",
    "\n",
    "# Pull data and perform Patient-level split to prevent data leakage\n",
    "df = get_metadata_from_bigquery(limit=500)\n",
    "unique_patients = df['patient_id'].unique()\n",
    "np.random.shuffle(unique_patients)\n",
    "split_idx = int(len(unique_patients) * 0.8)\n",
    "\n",
    "train_pts = unique_patients[:split_idx]\n",
    "train_df = df[df['patient_id'].isin(train_pts)].reset_index(drop=True)\n",
    "test_df = df[~df['patient_id'].isin(train_pts)].reset_index(drop=True)\n",
    "\n",
    "print(f\"üìä Training images: {len(train_df)} | Testing images: {len(test_df)}\")"
   ],
   "id": "74823876cbd52187",
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Access Denied: Table search:chc-nih-chest-xray.nih_chest_xray.metadata: User does not have permission to query table search:chc-nih-chest-xray.nih_chest_xray.metadata, or perhaps it does not exist.; reason: accessDenied, message: Access Denied: Table search:chc-nih-chest-xray.nih_chest_xray.metadata: User does not have permission to query table search:chc-nih-chest-xray.nih_chest_xray.metadata, or perhaps it does not exist.\n\nLocation: US\nJob ID: 6c028dd7-9105-4e88-b5cb-8b09628e5501\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mForbidden\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     11\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m bq_client.query(query).to_dataframe()\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# Pull data and perform Patient-level split to prevent data leakage\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m df = \u001B[43mget_metadata_from_bigquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlimit\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m500\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     15\u001B[39m unique_patients = df[\u001B[33m'\u001B[39m\u001B[33mpatient_id\u001B[39m\u001B[33m'\u001B[39m].unique()\n\u001B[32m     16\u001B[39m np.random.shuffle(unique_patients)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 11\u001B[39m, in \u001B[36mget_metadata_from_bigquery\u001B[39m\u001B[34m(limit)\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_metadata_from_bigquery\u001B[39m(limit=\u001B[32m200\u001B[39m):\n\u001B[32m      2\u001B[39m     \u001B[38;5;66;03m# This query extracts the filename and creates a binary label for Pneumonia\u001B[39;00m\n\u001B[32m      3\u001B[39m     query = \u001B[33mf\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[33m        SELECT\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[33m            image_id,\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m      9\u001B[39m \u001B[33m        LIMIT \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlimit\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbq_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Internship-prep/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py:2159\u001B[39m, in \u001B[36mQueryJob.to_dataframe\u001B[39m\u001B[34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001B[39m\n\u001B[32m   1929\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mto_dataframe\u001B[39m(\n\u001B[32m   1930\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1931\u001B[39m     bqstorage_client: Optional[\u001B[33m\"\u001B[39m\u001B[33mbigquery_storage.BigQueryReadClient\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1951\u001B[39m     ] = DefaultPandasDTypes.RANGE_TIMESTAMP_DTYPE,\n\u001B[32m   1952\u001B[39m ) -> \u001B[33m\"\u001B[39m\u001B[33mpandas.DataFrame\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1953\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001B[39;00m\n\u001B[32m   1954\u001B[39m \n\u001B[32m   1955\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   2157\u001B[39m \u001B[33;03m            :mod:`shapely` library cannot be imported.\u001B[39;00m\n\u001B[32m   2158\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2159\u001B[39m     query_result = \u001B[43mwait_for_query\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprogress_bar_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_results\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_results\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2160\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m query_result.to_dataframe(\n\u001B[32m   2161\u001B[39m         bqstorage_client=bqstorage_client,\n\u001B[32m   2162\u001B[39m         dtypes=dtypes,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2176\u001B[39m         range_timestamp_dtype=range_timestamp_dtype,\n\u001B[32m   2177\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Internship-prep/lib/python3.12/site-packages/google/cloud/bigquery/_tqdm_helpers.py:107\u001B[39m, in \u001B[36mwait_for_query\u001B[39m\u001B[34m(query_job, progress_bar_type, max_results)\u001B[39m\n\u001B[32m    103\u001B[39m progress_bar = get_progress_bar(\n\u001B[32m    104\u001B[39m     progress_bar_type, \u001B[33m\"\u001B[39m\u001B[33mQuery is running\u001B[39m\u001B[33m\"\u001B[39m, default_total, \u001B[33m\"\u001B[39m\u001B[33mquery\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    105\u001B[39m )\n\u001B[32m    106\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m progress_bar \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m107\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mquery_job\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_results\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_results\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    109\u001B[39m i = \u001B[32m0\u001B[39m\n\u001B[32m    110\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Internship-prep/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py:1773\u001B[39m, in \u001B[36mQueryJob.result\u001B[39m\u001B[34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001B[39m\n\u001B[32m   1768\u001B[39m     remaining_timeout = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1770\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m remaining_timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1771\u001B[39m     \u001B[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001B[39;00m\n\u001B[32m   1772\u001B[39m     \u001B[38;5;66;03m# long-running API, don't delay the next request at all.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mis_job_done\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m   1774\u001B[39m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1776\u001B[39m     \u001B[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001B[39;00m\n\u001B[32m   1777\u001B[39m     \u001B[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Internship-prep/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:294\u001B[39m, in \u001B[36mRetry.__call__.<locals>.retry_wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    290\u001B[39m target = functools.partial(func, *args, **kwargs)\n\u001B[32m    291\u001B[39m sleep_generator = exponential_sleep_generator(\n\u001B[32m    292\u001B[39m     \u001B[38;5;28mself\u001B[39m._initial, \u001B[38;5;28mself\u001B[39m._maximum, multiplier=\u001B[38;5;28mself\u001B[39m._multiplier\n\u001B[32m    293\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m294\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mretry_target\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    295\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    296\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_predicate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    297\u001B[39m \u001B[43m    \u001B[49m\u001B[43msleep_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    298\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    299\u001B[39m \u001B[43m    \u001B[49m\u001B[43mon_error\u001B[49m\u001B[43m=\u001B[49m\u001B[43mon_error\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    300\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Internship-prep/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:156\u001B[39m, in \u001B[36mretry_target\u001B[39m\u001B[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[32m    153\u001B[39m \u001B[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001B[39;00m\n\u001B[32m    154\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    155\u001B[39m     \u001B[38;5;66;03m# defer to shared logic for handling errors\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m156\u001B[39m     next_sleep = \u001B[43m_retry_error_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    157\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    158\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdeadline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[43m        \u001B[49m\u001B[43msleep_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[43m        \u001B[49m\u001B[43merror_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpredicate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    162\u001B[39m \u001B[43m        \u001B[49m\u001B[43mon_error\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexception_factory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    164\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    166\u001B[39m     \u001B[38;5;66;03m# if exception not raised, sleep before next attempt\u001B[39;00m\n\u001B[32m    167\u001B[39m     time.sleep(next_sleep)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Internship-prep/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:214\u001B[39m, in \u001B[36m_retry_error_helper\u001B[39m\u001B[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001B[39m\n\u001B[32m    208\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m predicate_fn(exc):\n\u001B[32m    209\u001B[39m     final_exc, source_exc = exc_factory_fn(\n\u001B[32m    210\u001B[39m         error_list,\n\u001B[32m    211\u001B[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001B[32m    212\u001B[39m         original_timeout,\n\u001B[32m    213\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m214\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m final_exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msource_exc\u001B[39;00m\n\u001B[32m    215\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m on_error_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    216\u001B[39m     on_error_fn(exc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Internship-prep/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:147\u001B[39m, in \u001B[36mretry_target\u001B[39m\u001B[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    146\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m         result = \u001B[43mtarget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    148\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m inspect.isawaitable(result):\n\u001B[32m    149\u001B[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/Internship-prep/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py:1722\u001B[39m, in \u001B[36mQueryJob.result.<locals>.is_job_done\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   1699\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m job_failed_exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1700\u001B[39m     \u001B[38;5;66;03m# Only try to restart the query job if the job failed for\u001B[39;00m\n\u001B[32m   1701\u001B[39m     \u001B[38;5;66;03m# a retriable reason. For example, don't restart the query\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1719\u001B[39m     \u001B[38;5;66;03m# into an exception that can be processed by the\u001B[39;00m\n\u001B[32m   1720\u001B[39m     \u001B[38;5;66;03m# `job_retry` predicate.\u001B[39;00m\n\u001B[32m   1721\u001B[39m     restart_query_job = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1722\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m job_failed_exception\n\u001B[32m   1723\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1724\u001B[39m     \u001B[38;5;66;03m# Make sure that the _query_results are cached so we\u001B[39;00m\n\u001B[32m   1725\u001B[39m     \u001B[38;5;66;03m# can return a complete RowIterator.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1731\u001B[39m     \u001B[38;5;66;03m# making any extra API calls if the previous loop\u001B[39;00m\n\u001B[32m   1732\u001B[39m     \u001B[38;5;66;03m# iteration fetched the finished job.\u001B[39;00m\n\u001B[32m   1733\u001B[39m     \u001B[38;5;28mself\u001B[39m._reload_query_results(\n\u001B[32m   1734\u001B[39m         retry=retry, **reload_query_results_kwargs\n\u001B[32m   1735\u001B[39m     )\n",
      "\u001B[31mForbidden\u001B[39m: 403 Access Denied: Table search:chc-nih-chest-xray.nih_chest_xray.metadata: User does not have permission to query table search:chc-nih-chest-xray.nih_chest_xray.metadata, or perhaps it does not exist.; reason: accessDenied, message: Access Denied: Table search:chc-nih-chest-xray.nih_chest_xray.metadata: User does not have permission to query table search:chc-nih-chest-xray.nih_chest_xray.metadata, or perhaps it does not exist.\n\nLocation: US\nJob ID: 6c028dd7-9105-4e88-b5cb-8b09628e5501\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def download_to_local(dataframe):\n",
    "    bucket = storage_client.bucket(\"gcs-public-data--healthcare-nih-chest-xray\")\n",
    "    count = 0\n",
    "    for img_id in dataframe['image_id']:\n",
    "        local_path = os.path.join(LOCAL_DATA_DIR, img_id)\n",
    "        if not os.path.exists(local_path):\n",
    "            blob = bucket.blob(f\"png/{img_id}\")\n",
    "            blob.download_to_filename(local_path)\n",
    "            count += 1\n",
    "    print(f\"‚úÖ Local Cache Updated. Downloaded {count} new images.\")\n",
    "\n",
    "# Sync local WSL storage with GCS references\n",
    "download_to_local(df)"
   ],
   "id": "b48eea3c5bbca7de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class NIHDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(LOCAL_DATA_DIR, row['image_id'])\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            # MONAI transforms expect (C, H, W)\n",
    "            image_np = np.array(image).astype(np.float32).transpose(2, 0, 1)\n",
    "            if self.transform:\n",
    "                image_np = self.transform(image_np)\n",
    "            return image_np, torch.tensor(row['label'], dtype=torch.long)\n",
    "        except Exception as e:\n",
    "            return torch.zeros((3, 224, 224)), torch.tensor(row['label'], dtype=torch.long)\n",
    "\n",
    "transforms = Compose([ScaleIntensity(), Resize((224, 224)), EnsureType()])\n",
    "train_loader = DataLoader(NIHDataset(train_df, transforms), batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(NIHDataset(test_df, transforms), batch_size=8, shuffle=False)\n",
    "\n",
    "print(\"‚úÖ Local Dataloaders Initialized.\")"
   ],
   "id": "65775ae8eaf57e5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate class weights for imbalance\n",
    "neg, pos = np.bincount(train_df['label'])\n",
    "weights = torch.tensor([1.0, neg/pos], dtype=torch.float).to(device)\n",
    "\n",
    "model = DenseNet121(spatial_dims=2, in_channels=3, out_channels=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "print(f\"‚úÖ Model ready with Class Weights: {weights.tolist()}\")"
   ],
   "id": "7d4119b64125d753",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.train()\n",
    "for epoch in range(5): # Increase for real training\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {i} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"üèÅ Training Finished.\")"
   ],
   "id": "7cb67f065a947c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_prediction(model, loader):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(loader))\n",
    "    img_tensor = images[0:1].to(device)\n",
    "\n",
    "    lgc = LayerGradCam(model, model.features)\n",
    "    attr = lgc.attribute(img_tensor, target=labels[0].item())\n",
    "    attr_upsampled = LayerGradCam.interpolate(attr, (224, 224))\n",
    "\n",
    "    img_display = img_tensor[0].cpu().numpy().transpose(1, 2, 0)\n",
    "    img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min() + 1e-8)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_display)\n",
    "    plt.title(f\"X-Ray (Label: {labels[0].item()})\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img_display)\n",
    "    plt.imshow(attr_upsampled.cpu().detach().numpy().squeeze(), cmap='jet', alpha=0.4)\n",
    "    plt.title(\"Grad-CAM Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_prediction(model, test_loader)"
   ],
   "id": "469533393981f621",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
